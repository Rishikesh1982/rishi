{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1)\n",
        "\n",
        "**Forward Propagation in a Neural Network**\n",
        "\n",
        "Forward propagation is the process through which input data passes through a neural network to generate predictions or output. During this process:\n",
        "\n",
        "The input data is multiplied by the weights of the connections.\n",
        "\n",
        "Bias terms are added to adjust the output.\n",
        "\n",
        "The result is passed through activation functions to introduce non-linearity and control the output.\n",
        "\n",
        "2)\n",
        "\n",
        "**Purpose of the Activation Function in Forward Propagation**\n",
        "\n",
        "The activation function introduces non-linearity to the network, enabling it to learn and model complex patterns in data. Without activation functions, the network would behave like a linear model, irrespective of the number of layers, and lose its ability to solve non-linear problems.\n",
        "\n",
        "3)\n",
        "\n",
        "**Steps Involved in the Backpropagation Algorithm**\n",
        "\n",
        "Backpropagation is a method for training neural networks by minimizing the error (loss) between predicted and actual outputs. It involves the following steps:\n",
        "\n",
        "Forward Pass: Calculate the predicted output using forward propagation.\n",
        "\n",
        "Compute Loss: Measure the error using a loss function (e.g., Mean Squared Error or Cross-Entropy Loss).\n",
        "\n",
        "Backward Pass:\n",
        "Compute the gradient of the loss with respect to each weight and bias using the chain rule.\n",
        "\n",
        "Propagate these gradients layer by layer, starting from the output layer and moving backward to the input layer.\n",
        "\n",
        "4)\n",
        "\n",
        "**Purpose of the Chain Rule in Backpropagation**\n",
        "\n",
        "The chain rule is essential in backpropagation for computing gradients of the loss function with respect to weights and biases in earlier layers. It allows the decomposition of the overall gradient into manageable parts (layer-wise gradients), ensuring efficient error propagation through the network.\n",
        "\n",
        "5)"
      ],
      "metadata": {
        "id": "5thdoQcXBFL_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jUffV7MBEOe",
        "outputId": "c2579fab-3bfe-48a7-b1eb-35ae0a42b02b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output of the network: [[0.34876531]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define activation function (ReLU and Sigmoid)\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Forward propagation function\n",
        "def forward_propagation(X, weights, biases):\n",
        "    # Input to hidden layer\n",
        "    Z1 = np.dot(X, weights['W1']) + biases['b1']\n",
        "    A1 = relu(Z1)\n",
        "\n",
        "    # Hidden to output layer\n",
        "    Z2 = np.dot(A1, weights['W2']) + biases['b2']\n",
        "    A2 = sigmoid(Z2)\n",
        "\n",
        "    return A2, {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}\n",
        "\n",
        "# Sample data\n",
        "X = np.array([[0.5, 0.2, 0.1]])  # Example input\n",
        "weights = {\n",
        "    \"W1\": np.random.randn(3, 4),  # 3 inputs to 4 hidden units\n",
        "    \"W2\": np.random.randn(4, 1),  # 4 hidden units to 1 output\n",
        "}\n",
        "biases = {\n",
        "    \"b1\": np.zeros((1, 4)),  # Bias for hidden layer\n",
        "    \"b2\": np.zeros((1, 1)),  # Bias for output layer\n",
        "}\n",
        "\n",
        "# Run forward propagation\n",
        "output, cache = forward_propagation(X, weights, biases)\n",
        "print(\"Output of the network:\", output)\n"
      ]
    }
  ]
}