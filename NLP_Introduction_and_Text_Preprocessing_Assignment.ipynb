{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1) What is the primary goal of Natural Language Processing (NLP)?\n",
        "\n",
        "The primary goal of NLP is to enable computers to understand, interpret, and generate human language in a way that is both meaningful and useful.\n",
        "\n",
        "2) What does \"tokenization\" refer to in text processing?\n",
        "\n",
        "Tokenization is the process of breaking down a text into smaller units called tokens. These tokens can be words, phrases, symbols, or other meaningful elements.\n",
        "\n",
        "3) What is the difference between lemmatization and stemming?\n",
        "\n",
        "Both lemmatization and stemming aim to reduce words to their base form. Stemming does this through simple heuristics (e.g., removing suffixes), which can sometimes result in non-words. Lemmatization uses vocabulary and morphological analysis to find the lemma (dictionary form) of a word, ensuring it's a valid word.\n",
        "\n",
        "4) What is the role of regular expressions (regex) in text processing?\n",
        "\n",
        "Regular expressions are powerful tools for pattern matching in text. They can be used for tasks like searching, replacing, validating, and extracting specific text patterns.\n",
        "\n",
        "5) What is Word2vec and how does it represent words in a vector space?\n",
        "\n",
        "Word2vec is a technique for creating word embeddings. It represents words as dense vectors in a high-dimensional space, where words with similar meanings are located closer to each other.\n",
        "\n",
        "6) How does frequency distribution help in text analysis?\n",
        "\n",
        "Frequency distribution shows how often each word or token appears in a text. This information can be used to identify important keywords, understand the topic of the text, and perform other analyses.\n",
        "\n",
        "7) Why is text normalization important in NLP?\n",
        "\n",
        "Text normalization aims to transform text into a more consistent and usable form. This includes tasks like lowercasing, removing punctuation, handling contractions, and correcting spelling errors. It's important because it reduces noise and improves the performance of NLP models.\n",
        "\n",
        "8) What is the difference between sentence tokenization and word tokenization?\n",
        "\n",
        "Sentence tokenization splits a text into individual sentences, while word tokenization splits a text into individual words or tokens.\n",
        "\n",
        "9) What are co-occurrence vectors in NLP?\n",
        "\n",
        "Co-occurrence vectors represent words based on how often they appear together within a specific context (e.g., a sentence or a document). They capture semantic relationships between words based on their usage.\n",
        "\n",
        "10) What is the significance of lemmatization in improving NLP tasks?\n",
        "\n",
        "Lemmatization helps improve NLP tasks by reducing words to their base form, which reduces data sparsity and improves the accuracy of models that rely on word counts or frequencies.\n",
        "\n",
        "11) What is the primary use of word embeddings in NLP?\n",
        "\n",
        "The primary use of word embeddings is to represent words in a way that captures their semantic meaning and relationships, which can then be used as input features for various NLP models.\n",
        "\n",
        "12) What is an annotator in NLP?\n",
        "\n",
        "An annotator is a person who labels or tags data for NLP tasks, such as part-of-speech tagging, named entity recognition, or sentiment analysis.\n",
        "\n",
        "13)What are the key steps in text processing before applying machine learning models?\n",
        "\n",
        "Key steps include:\n",
        "\n",
        "Text cleaning: Removing noise like HTML tags, special characters, etc.\n",
        "\n",
        "Tokenization: Breaking text into tokens.\n",
        "\n",
        "Normalization: Lowercasing, stemming/lemmatization, etc.\n",
        "\n",
        "Feature extraction: Converting text into numerical features (e.g., TF-IDF, word embeddings).\n",
        "\n",
        "14) What is the history of NLP and how has it evolved?\n",
        "\n",
        "NLP has evolved from early rule-based systems to statistical methods and, more recently, deep learning approaches. Key milestones include the development of parsing techniques, statistical language models, and neural network-based models like recurrent neural networks (RNNs) and transformers.\n",
        "\n",
        "15) Why is sentence processing important in NLP?\n",
        "\n",
        "Sentence processing is important for understanding the structure and meaning of text at the sentence level, which is crucial for tasks like machine translation, question answering, and text summarization.\n",
        "\n",
        "16) How do word embeddings improve the understanding of language semantics in NLP?\n",
        "\n",
        "Word embeddings capture semantic relationships between words by representing them as vectors in a continuous space. This allows models to understand word similarity, analogy, and other semantic properties.\n",
        "\n",
        "17) How does the frequency distribution of words help in text classification?\n",
        "\n",
        "Frequency distribution can be used to identify keywords that are indicative of specific categories or classes, which can then be used to train text classification models.\n",
        "\n",
        "18) What are the advantages of using regex in text cleaning?\n",
        "\n",
        "Regex provides a flexible and powerful way to define complex patterns for cleaning and manipulating text, such as removing specific characters, validating formats, and extracting information.\n",
        "\n",
        "19) What is the difference between Word2vec and Doc2vec?\n",
        "\n",
        "Word2vec creates embeddings for individual words, while Doc2vec (also known as Paragraph Vector) creates embeddings for entire documents or paragraphs.\n",
        "\n",
        "20) Why is understanding text normalization important in NLP?\n",
        "\n",
        "Understanding text normalization is important because it allows you to choose the appropriate normalization techniques for your specific NLP task and data.\n",
        "\n",
        "21) How does word count help in text analysis?\n",
        "\n",
        "Word count can be used to identify important terms, analyze text complexity, and perform basic text summarization.\n",
        "\n",
        "22) How does lemmatization help in NLP tasks like search engines and chatbots?\n",
        "\n",
        "Lemmatization helps improve search relevance by matching different forms of a word (e.g., \"running,\" \"runs,\" \"ran\") to its base form (\"run\"). In chatbots, it helps understand user input more accurately.\n",
        "\n",
        "23) What is the purpose of using Doc2vec in text processing?\n",
        "\n",
        "The purpose of Doc2vec is to create vector representations of documents, which can be used for tasks like document similarity, clustering, and classification.\n",
        "\n",
        "24) What is the importance of sentence processing in NLP?\n",
        "\n",
        "Sentence processing is essential for tasks that require understanding the relationships between different parts of a sentence, such as parsing, semantic role labeling, and machine translation.\n",
        "\n",
        "25) What is text normalization, and what are the common techniques used in it?\n",
        "\n",
        "Text normalization is the process of transforming text into a more consistent form. Common techniques include:\n",
        "\n",
        "Lowercasing: Converting all text to lowercase.\n",
        "\n",
        "Punctuation removal: Removing punctuation marks.\n",
        "\n",
        "Stop word removal: Removing common words like \"the,\" \"a,\" \"is.\"\n",
        "\n",
        "Stemming/Lemmatization: Reducing words to their base form.\n",
        "\n",
        "Handling contractions: Expanding contractions (e.g., \"don't\" to \"do not\").\n",
        "\n",
        "26) Why is word tokenization important in NLP?\n",
        "\n",
        "Word tokenization is important because it breaks down text into manageable units that can be processed by NLP models.\n",
        "\n",
        "27)How does sentence tokenization differ from word tokenization in NLP?\n",
        "\n",
        "Sentence tokenization splits text into sentences, while word tokenization splits text into individual words or tokens within those sentences.\n",
        "\n",
        "28) What is the primary purpose of text processing in NLP?\n",
        "\n",
        "The primary purpose of text processing is to prepare text data for analysis and use in NLP models.\n",
        "\n",
        "29) What are the key challenges in NLP?\n",
        "\n",
        "Key challenges include:\n",
        "\n",
        "Ambiguity: Natural language is often ambiguous.\n",
        "\n",
        "Context: Understanding the meaning of words and sentences depends on context.\n",
        "\n",
        "Sarcasm and irony: Detecting these can be difficult.\n",
        "\n",
        "Handling different languages and dialects.\n",
        "\n",
        "Data sparsity: Some words or phrases may occur infrequently.\n",
        "\n",
        "30) How do co-occurrence vectors represent relationships between words?\n",
        "\n",
        "They represent relationships by counting how often words appear together in a given context. Words that frequently co-occur are considered to be related.\n",
        "\n",
        "31) What is the role of frequency distribution in text analysis?\n",
        "\n",
        "It helps identify important terms, understand topic distribution, and perform basic text summarization.\n",
        "\n",
        "32) What is the impact of word embeddings on NLP tasks?\n",
        "\n",
        "Word embeddings significantly improve the performance of many NLP tasks by capturing semantic relationships between words, leading to better understanding of meaning and context.\n",
        "\n",
        "33) What is the purpose of using lemmatization in text preprocessing?\n",
        "\n",
        "The purpose is to reduce words to their base forms, which reduces data sparsity, improves model accuracy, and simplifies analysis"
      ],
      "metadata": {
        "id": "-DmO4sLH5RgI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "text = \"Hello, how are you doing today?\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "B2_d2viR-ASx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text = \"Hello world! It's a beautiful day. Let's learn NLP.\"\n",
        "sentences = sent_tokenize(text)\n",
        "print(sentences)\n"
      ],
      "metadata": {
        "id": "DaCnI9Qr-CDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "text = \"This is an example showing off stop word filtration.\"\n",
        "tokens = word_tokenize(text)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_sentence = [word for word in tokens if word.lower() not in stop_words]\n",
        "print(filtered_sentence)\n"
      ],
      "metadata": {
        "id": "WvSHXZHF8jM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "words = [\"running\", \"runner\", \"ran\", \"easily\", \"fairly\"]\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "print(stemmed_words)\n"
      ],
      "metadata": {
        "id": "_8WZtpWu8jKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = [\"running\", \"better\", \"geese\", \"studies\"]\n",
        "lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
        "print(lemmatized_words)\n"
      ],
      "metadata": {
        "id": "LoAe_SVU8jIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "text = \"Hello, World! NLP is amazing.\"\n",
        "normalized_text = ''.join([char.lower() for char in text if char not in string.punctuation])\n",
        "print(normalized_text)\n"
      ],
      "metadata": {
        "id": "F22V_jW88jE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "\n",
        "corpus = [\"I love NLP\", \"NLP is fun\", \"I enjoy learning NLP\"]\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "co_occurrence_matrix = (X.T @ X).toarray()\n",
        "print(co_occurrence_matrix)\n"
      ],
      "metadata": {
        "id": "KNPNK-v48jAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"Contact us at support@example.com or sales@example.org\"\n",
        "emails = re.findall(r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+', text)\n",
        "print(emails)\n"
      ],
      "metadata": {
        "id": "sRoRTxWz8i-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "sentences = [[\"I\", \"love\", \"NLP\"], [\"NLP\", \"is\", \"fun\"], [\"I\", \"enjoy\", \"learning\"]]\n",
        "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "print(model.wv['NLP'])\n"
      ],
      "metadata": {
        "id": "6nF4iY568i7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "\n",
        "documents = [TaggedDocument(words=[\"I\", \"love\", \"NLP\"], tags=[\"doc1\"]),\n",
        "             TaggedDocument(words=[\"NLP\", \"is\", \"fun\"], tags=[\"doc2\"])]\n",
        "model = Doc2Vec(documents, vector_size=100, window=5, min_count=1, workers=4)\n",
        "print(model.dv[\"doc1\"])\n"
      ],
      "metadata": {
        "id": "K6Lq56sz8i43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "text = \"I am learning NLP\"\n",
        "tokens = word_tokenize(text)\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "print(pos_tags)\n"
      ],
      "metadata": {
        "id": "BPqhEpvv8iPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "sent1 = \"I love NLP\"\n",
        "sent2 = \"I enjoy learning NLP\"\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform([sent1, sent2])\n",
        "similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
        "print(similarity)\n"
      ],
      "metadata": {
        "id": "_vBcSx8m8h3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = \"Barack Obama was born in Hawaii.\"\n",
        "doc = nlp(text)\n",
        "for entity in doc.ents:\n",
        "    print(entity.text, entity.label_)\n"
      ],
      "metadata": {
        "id": "qtuZGPAE8hki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_document(text, chunk_size):\n",
        "    words = text.split()\n",
        "    return [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
        "\n",
        "text = \"This is a long document that needs to be split into smaller chunks for easier processing.\"\n",
        "chunks = split_document(text, 5)\n",
        "print(chunks)\n"
      ],
      "metadata": {
        "id": "jWD2VSf59Nop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "corpus = [\"I love NLP\", \"NLP is fun\", \"I enjoy learning NLP\"]\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "print(tfidf_matrix.toarray())\n"
      ],
      "metadata": {
        "id": "MZL3Az6D9Nh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "text = \"This is an example of applying multiple preprocessing steps at once.\"\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "processed_tokens = [stemmer.stem(word) for word in tokens if word.lower() not in stop_words]\n",
        "print(processed_tokens)\n"
      ],
      "metadata": {
        "id": "PVdkhh4S9Nco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.probability import FreqDist\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "text = \"This is a sample text with several words. This text is for NLP.\"\n",
        "tokens = word_tokenize(text)\n",
        "freq_dist = FreqDist(tokens)\n",
        "freq_dist.plot(30, cumulative=False)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "po8O3Ryj9NT3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}