{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " 1)\n",
        "\n",
        "Activation functions determine whether a neuron should be activated based on its input. They introduce non-linearity into the model, enabling neural networks to learn and model complex data patterns.\n",
        "\n",
        "Linear Activation Functions:\n",
        "\n",
        "These perform a linear transformation\n",
        "\n",
        "Limitation: Linear functions cannot model complex data since they do not introduce non-linearity. Regardless of the number of layers, the output is a linear combination of inputs.\n",
        "Nonlinear Activation Functions:\n",
        "\n",
        "Examples include ReLU, Sigmoid, and Tanh.\n",
        "\n",
        "They enable the model to approximate complex functions and solve problems like image classification or natural language processing.\n",
        "\n",
        "Advantage: Nonlinearity allows deep networks to learn hierarchical features.\n",
        "\n",
        "Hidden layers are responsible for extracting features from data. Without nonlinearity, the networks capability to model complex relationships is limited. Nonlinear functions allow multiple layers to work together in extracting and learning intricate patterns.\n",
        "\n",
        " 2)\n",
        "\n",
        "**Sigmoid Activation Function**\n",
        "\n",
        "The Sigmoid activation function is defined as:\n",
        "\n",
        "f(x)= 1/1+e âˆ’x\n",
        "\n",
        "Characteristics:\n",
        "\n",
        "Range: The output of the sigmoid function lies between\n",
        "0\n",
        "0 and\n",
        "1\n",
        "1.\n",
        "\n",
        "Shape: It has an \"S-shaped\" curve (also called a logistic curve).\n",
        "\n",
        "Gradient: The derivative is largest near\n",
        "ğ‘¥\n",
        "=\n",
        "0\n",
        "x=0, but it decreases rapidly as\n",
        "ğ‘¥\n",
        "x moves away from 0.\n",
        "\n",
        "Common Usage:\n",
        "\n",
        "Binary Classification: Frequently used in the output layer of binary classification tasks, where the output is interpreted as a probability (e.g., spam vs. not spam).\n",
        "\n",
        "Rarely used in hidden layers due to the vanishing gradient problem, which makes training deep networks difficult.\n",
        "\n",
        "**Rectified Linear Unit (ReLU) Activation Function**\n",
        "\n",
        "The ReLU activation function is defined as:\n",
        "\n",
        "f(x)=max(0,x)\n",
        "\n",
        "Characteristics:\n",
        "\n",
        "Outputs\n",
        "ğ‘¥\n",
        "x if\n",
        "ğ‘¥\n",
        ">\n",
        "0\n",
        "x>0, otherwise outputs\n",
        "0\n",
        "0.\n",
        "\n",
        "Non-linearity: Introduces non-linear behavior, enabling networks to model complex relationships.\n",
        "\n",
        "Efficiency: ReLU is computationally efficient as it involves simple thresholding.\n",
        "\n",
        "Advantages:\n",
        "Avoids Vanishing Gradient: Gradients remain significant for positive inputs, allowing effective backpropagation.\n",
        "\n",
        "Sparsity: Outputs zero for negative values, resulting in sparse activation, which improves computational efficiency.\n",
        "\n",
        "Efficient Computation: Simple and fast to compute.\n",
        "\n",
        "Challenges:\n",
        "\n",
        "Dying ReLU Problem: If neurons consistently output zero (due to negative weights), they may \"die\" and stop learning altogether. This occurs because their gradients are always zero.\n",
        "\n",
        "Not Zero-Centered: The output is non-symmetric around zero, which can slow down optimization.\n",
        "\n",
        "**Tanh Activation Function**\n",
        "\n",
        "The Tanh activation function (hyperbolic tangent) is defined as:\n",
        "\n",
        "f(x) = e^x - e^-x / e^x + e^-x\n",
        "\n",
        "Characteristics:\n",
        "\n",
        "Range: Outputs values between\n",
        "âˆ’\n",
        "1\n",
        "âˆ’1 and\n",
        "1\n",
        "1.\n",
        "\n",
        "Shape: It is also an \"S-shaped\" curve but symmetric around the origin.\n",
        "\n",
        "Zero-Centered: Tanh produces outputs with zero mean, which can help with optimization by centering data closer to zero.\n",
        "\n",
        "Purpose:\n",
        "\n",
        "Used in hidden layers to normalize inputs to the next layer, especially when data is zero-centered.\n",
        "\n",
        "Improves learning efficiency compared to Sigmoid because gradients are steeper.\n",
        "Comparison Between Tanh and Sigmoid\n",
        "\n",
        "Range:\n",
        "\n",
        "Sigmoid: Outputs between\n",
        "0\n",
        "0 and\n",
        "1\n",
        "1.\n",
        "\n",
        "Tanh: Outputs between\n",
        "âˆ’\n",
        "1\n",
        "âˆ’1 and\n",
        "1\n",
        "1.\n",
        "\n",
        "Zero-Centered:\n",
        "\n",
        "Sigmoid: Not zero-centered, which can lead to slower learning.\n",
        "Tanh: Zero-centered, making it more suitable for zero-centered data.\n",
        "Gradient Magnitude:\n",
        "\n",
        "Tanh has steeper gradients than Sigmoid, making it more effective in propagating information during backpropagation.\n",
        "\n",
        "3)\n",
        "\n",
        "**Significance of Activation Functions in Hidden Layers**\n",
        "\n",
        "Activation functions are crucial in hidden layers because:\n",
        "\n",
        "They allow the network to model nonlinear relationships, making it capable of solving complex tasks like image recognition or natural language processing.\n",
        "\n",
        "Without them, the network would behave like a linear model, no matter how deep the architecture is.\n",
        "\n",
        "Activation functions enable the network to learn from errors, adapt weights effectively, and capture important patterns in data.\n",
        "\n",
        "4)\n",
        "\n",
        "1. Classification Problems\n",
        "\n",
        "Classification tasks involve predicting discrete categories or class labels. The activation function ensures the output is in a format suitable for classification (e.g., probabilities).\n",
        "\n",
        "a) Binary Classification:\n",
        "\n",
        "Activation Function: Sigmoid\n",
        "\n",
        "Reason: Sigmoid squashes the output to a range between 0 and 1, making it interpretable as a probability for a single class.\n",
        "\n",
        "Example: Predicting whether an email is spam or not spam.\n",
        "\n",
        "Output: A single neuron outputs a probability, which can be thresholded (e.g.,\n",
        ">\n",
        "0.5\n",
        ">0.5 for one class,\n",
        "â‰¤\n",
        "0.5\n",
        "â‰¤0.5 for the other).\n",
        "\n",
        "b) Multi-Class Classification:\n",
        "\n",
        "Activation Function: Softmax\n",
        "\n",
        "Reason: Softmax converts raw scores (logits) into a probability distribution across multiple classes, ensuring the probabilities sum to 1.\n",
        "\n",
        "Example: Classifying images into categories like \"cat,\" \"dog,\" or \"bird.\"\n",
        "\n",
        "Output: Each output neuron corresponds to a class, and the neuron with the highest probability represents the predicted class.\n",
        "\n",
        "2. Regression Problems\n",
        "\n",
        "Regression tasks involve predicting continuous numeric values, such as house prices or stock prices. Here, the output layer must produce raw, unbounded numeric values.\n",
        "\n",
        "Activation Function: Linear (No Activation)\n",
        "\n",
        "Reason: A linear activation function outputs raw numeric values without any transformation, which is ideal for continuous data.\n",
        "\n",
        "Example: Predicting house prices, where the output is an exact dollar value.\n",
        "\n",
        "Output: The output neuron directly represents the predicted value."
      ],
      "metadata": {
        "id": "egisf2ab84Kk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Generate synthetic data\n",
        "X, y = make_regression(n_samples=1000, n_features=10, noise=0.1)\n",
        "y = (y - y.min()) / (y.max() - y.min())  # Normalize target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale input data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Function to build and compile models\n",
        "def build_model(activation):\n",
        "    model = Sequential([\n",
        "        Dense(32, activation=activation, input_shape=(X_train.shape[1],)),\n",
        "        Dense(16, activation=activation),\n",
        "        Dense(1, activation='linear')  # Output layer for regression\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(learning_rate=0.01), loss='mse', metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "# Train and compare models\n",
        "activations = ['relu', 'sigmoid', 'tanh']\n",
        "for act in activations:\n",
        "    model = build_model(act)\n",
        "    print(f\"\\nTraining model with {act} activation:\")\n",
        "    model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1, validation_data=(X_test, y_test))\n",
        "    loss, mae = model.evaluate(X_test, y_test, verbose=0)\n",
        "    print(f\"{act.capitalize()} Activation - Test Loss: {loss:.4f}, Test MAE: {mae:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYs8X9BiBJkS",
        "outputId": "471bfdf9-3195-4be5-98d4-d9217a11e0c7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training model with relu activation:\n",
            "Epoch 1/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - loss: 0.1780 - mae: 0.3142 - val_loss: 0.0226 - val_mae: 0.1198\n",
            "Epoch 2/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0237 - mae: 0.1128 - val_loss: 0.0108 - val_mae: 0.0810\n",
            "Epoch 3/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0076 - mae: 0.0698 - val_loss: 0.0064 - val_mae: 0.0650\n",
            "Epoch 4/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0046 - mae: 0.0523 - val_loss: 0.0042 - val_mae: 0.0512\n",
            "Epoch 5/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0032 - mae: 0.0449 - val_loss: 0.0031 - val_mae: 0.0437\n",
            "Epoch 6/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0022 - mae: 0.0366 - val_loss: 0.0025 - val_mae: 0.0394\n",
            "Epoch 7/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0017 - mae: 0.0320 - val_loss: 0.0020 - val_mae: 0.0342\n",
            "Epoch 8/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0013 - mae: 0.0277 - val_loss: 0.0017 - val_mae: 0.0320\n",
            "Epoch 9/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 9.9282e-04 - mae: 0.0251 - val_loss: 0.0015 - val_mae: 0.0297\n",
            "Epoch 10/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 8.2083e-04 - mae: 0.0231 - val_loss: 0.0014 - val_mae: 0.0285\n",
            "Relu Activation - Test Loss: 0.0014, Test MAE: 0.0285\n",
            "\n",
            "Training model with sigmoid activation:\n",
            "Epoch 1/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.0357 - mae: 0.1500 - val_loss: 0.0017 - val_mae: 0.0346\n",
            "Epoch 2/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0019 - mae: 0.0333 - val_loss: 2.8894e-04 - val_mae: 0.0110\n",
            "Epoch 3/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5352e-04 - mae: 0.0089 - val_loss: 1.4016e-04 - val_mae: 0.0080\n",
            "Epoch 4/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.6579e-05 - mae: 0.0068 - val_loss: 1.5386e-04 - val_mae: 0.0086\n",
            "Epoch 5/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.4095e-05 - mae: 0.0073 - val_loss: 1.3126e-04 - val_mae: 0.0072\n",
            "Epoch 6/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.3026e-05 - mae: 0.0063 - val_loss: 1.1803e-04 - val_mae: 0.0072\n",
            "Epoch 7/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.9377e-05 - mae: 0.0070 - val_loss: 1.3797e-04 - val_mae: 0.0077\n",
            "Epoch 8/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7.6115e-05 - mae: 0.0065 - val_loss: 1.3986e-04 - val_mae: 0.0083\n",
            "Epoch 9/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.9887e-05 - mae: 0.0072 - val_loss: 1.1640e-04 - val_mae: 0.0071\n",
            "Epoch 10/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7.3476e-05 - mae: 0.0064 - val_loss: 1.2354e-04 - val_mae: 0.0066\n",
            "Sigmoid Activation - Test Loss: 0.0001, Test MAE: 0.0066\n",
            "\n",
            "Training model with tanh activation:\n",
            "Epoch 1/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 0.1156 - mae: 0.2608 - val_loss: 0.0227 - val_mae: 0.1195\n",
            "Epoch 2/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0139 - mae: 0.0937 - val_loss: 0.0080 - val_mae: 0.0699\n",
            "Epoch 3/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0048 - mae: 0.0554 - val_loss: 0.0041 - val_mae: 0.0485\n",
            "Epoch 4/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0032 - mae: 0.0446 - val_loss: 0.0028 - val_mae: 0.0395\n",
            "Epoch 5/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0024 - mae: 0.0390 - val_loss: 0.0027 - val_mae: 0.0398\n",
            "Epoch 6/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0016 - mae: 0.0306 - val_loss: 0.0021 - val_mae: 0.0351\n",
            "Epoch 7/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0013 - mae: 0.0281 - val_loss: 0.0019 - val_mae: 0.0338\n",
            "Epoch 8/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0011 - mae: 0.0276 - val_loss: 0.0012 - val_mae: 0.0260\n",
            "Epoch 9/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.9031e-04 - mae: 0.0224 - val_loss: 0.0011 - val_mae: 0.0238\n",
            "Epoch 10/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7.1538e-04 - mae: 0.0214 - val_loss: 0.0011 - val_mae: 0.0246\n",
            "Tanh Activation - Test Loss: 0.0011, Test MAE: 0.0246\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5)\n",
        "\n",
        " After comparing their effects on convergence and performance\n",
        "\n",
        "ReLU: Fast convergence, generally better for deep networks.\n",
        "\n",
        "Sigmoid: Slower convergence due to vanishing gradients.\n",
        "\n",
        "Tanh: Performs better than Sigmoid but still slower than ReLU."
      ],
      "metadata": {
        "id": "GUrZTdwCBt4O"
      }
    }
  ]
}